<head>
<title>
Designing for Testability
</title>

<meta 
	name="author" 
	lang="en" 
	content="Kevin Garwood">
<meta 
	name="keywords" 
	lang="en" 
	content="Rapid Inquiry Facility, RIF, environmental health, Kevin Garwood, Java, testability">
	
<link rel="stylesheet" href="../rifDesignManual.css" type="text/css">
</head>

<body>



<header>
<!-- Top Banner -->


<table bgcolor="#B9CDE5">

<tr>
<td width="1000">
<img src="../img/RIFMainBanner.jpg">
</td>
</tr>

<tr>
<td width="1000">

<script src="../lib/jquery-2.1.4.min.js" type="text/javascript"></script>


<script type="text/javascript">
var timeout         = 500;
var closetimer		= 0;
var ddmenuitem      = 0;

function rif_open()
{	rif_canceltimer();
	rif_close();
	ddmenuitem = $(this).find('ul').eq(0).css('visibility', 'visible');}

function rif_close()
{	if(ddmenuitem) ddmenuitem.css('visibility', 'hidden');}

function rif_timer()
{	closetimer = window.setTimeout(rif_close, timeout);}

function rif_canceltimer()
{	if(closetimer)
	{	window.clearTimeout(closetimer);
		closetimer = null;}}

$(document).ready(function()
{	$('#rif > li').bind('mouseover', rif_open);
	$('#rif > li').bind('mouseout',  rif_timer);});

document.onclick = rif_close;
</script>

<ul id="rif">
	<li>
		<a href="../index.html">Project Background</a>
	</li>
	<li>
		<a href="../tools/Tools.html">Tool Suite</a>
	</li>
	<li>
		<a href="../general_architecture/GeneralArchitecture.html">General Architecture</a>
	</li>
	<li>
		<a href="../front_ends/FrontEnds.html">Front Ends</a>
	</li>
	<li>
		<a href="./Middleware.html">Middleware</a>
	</li>		
	<li><a href="../back_ends/BackEnds.html">Back Ends</a>
	</li>
	<li><a href="../about_us/AboutUs.html">About Us</a>
	</li>
</ul>

<div class="clear"> </div>
<br>

</td>
</tr>
</table>

</header>


<nav id="sideBarNavigationLinks">
<!-- Left Side Bar -->

<a href="./CodingPhilosophy.html">Coding Philosophy</a>
<br>
<a href="./SummaryOfDesignDecisions.html">Summary of Design Decisions</a>
<br>
<a href="./GeneralArchitecture.html">General Architecture</a>
<ul>
<li>
	<a href="./PresentationLayer.html">Presentation Layer</a>
</li>
<li>
	<a href="./BusinessConceptLayer.html">Business Concept layer</a>
</li>
<li>
	<a href="./DataStorageLayer.html">Data Storage Layer</a>
</li>
</ul>

<a href="./WebServices.html">Web Services</a>
<br>
<a href="./FileFormats.html">Support for File Formats</a>
<br>
<b>Design Aspects</b>
	<ul>
	<li>
		<a href="./DesigningForSecurity.html">Security</a>
	</li>
	<li>
		<a href="./DesigningForConcurrency.html">Concurrency</a>
	</li>
	<li>
		<a href="./DesigningForTestability.html">Testability</a>
	</li>
	<li>
		<a href="./DesigningForValidation.html">Validation</a>
	</li>
	<li>
		<a href="./DesigningForMaintenance.html">Maintenance</a>
	</li>
	<li>
		<a href="./ErrorHandling.html">Error Handling</a>
	</li>
	</ul>

<a href="./CodeRoadMap.html">Code Road Map</a>
<br>
<a href="./CodingConventions.html">Coding Conventions</a>

</nav>

<section>
<!-- Main Content Area -->


<h1>Designing for Testability</h1>

<p>
<i>
by <a href="mailto:kgarwood@users.sourceforge.net">Kevin Garwood</a>
</i>
</p>

<h2>
Limiting testing efforts
</h2>
The first step in designing for testability is to limit the scope of our testing effort.
Two general design decisions allow us to define what areas we'll test:



<blockquote>
<font color="green">
<i>
<b>
<a href="SummaryOfDesignDecisions.html#gen_design4">
General Design-4
</a>
</b>: 
Wherever possible, limit the paths of execution that are likely to occur.
</i>
</font>
</blockquote>

<blockquote>
<font color="green">
<i>
<b>
<a href="SummaryOfDesignDecisions.html#gen_design7">
General Design-7
</a>
</b>: 
Encapsulate business concept and data storage layers of the architecture through service APIs.  
Do not allow clients to know which class is implementing the service interfaces.
</i>
</font>
</blockquote>

<p>
Instead of trying to test every public method of every class, we will restrict testing to
business classes and service interfaces.

<blockquote>
<font color="green">
<i>
<b>
<a href="SummaryOfDesignDecisions.html#testability1">
Testability-1
</a>
</b>: 
Test coverage will be limited to public methods of classes and service interfaces that are
defined within the business concept layer.
</i>
</font>
</blockquote>

<p>
Next, we should identify what kinds of test scenarios we want to cover.  It is useful to
divide test cases into the types of behaviour they exhibit.  The following categories were
used to classify all test cases:
<ul>
<li>
<b>common</b>: should demonstrate a typical valid use a method.
</li>

<li>
<b>uncommon</b>: will demonstrate boundary test case scenarios that are unusual but still valid.
Unusual behaviour could relate to testing for boundary edge conditions or using a list of one item.
</li>

<li>
<b>error</b>: these test cases are intentionally designed to cause the software to generate
an exception.
</li>

<li>
<b>malicious</b>: these test cases are error test cases that are specifically designed to test
the ability of the software to respond to malicious code attacks.
</li>
</ul>
</p>

<blockquote>
<font color="green">
<i>
<b>
<a href="SummaryOfDesignDecisions.html#testability2">
Testability-2
</a>
</b>: 
Test scenarios will be divided based on the following categories: common, uncommon, error, 
and malicious.
</i>
</font>
</blockquote>

<p>
Testing for uncommon behaviour greatly depends on the nature of the test data set that is 
managed in the RIF database.  Currently, the data set is designed to demonstrate RIF features
to scientists rather than to be rich enough to provide cases where zero or one result might
be returned.  Until the data set becomes more complex, we should defer testing this category
of test scenario.
</p>

<blockquote>
<font color="green">
<i>
<b>
<a href="SummaryOfDesignDecisions.html#testability3">
Testability-3
</a>
</b>: 
Defer testing for uncommon behaviour until we build a more diverse
test data set that could return unusual results.
</i>
</font>
</blockquote>




<h3>
Minimising test scenarios that consider the order in which service methods are called
</h3>
For this section, we're going to develop an approach for creating test cases that test 
<code>rifServices.businessConceptLayer.RIFStudySubmissionAPI</code> and
<code>rifServices.businessConceptLayer.RIFStudyResultRetrievalAPI</code>.

<p>
Accounting for the future, there are at least 35 methods that are part of the study submission service
and at least 32 methods that are part of the study result retrieval service.  All considered, there at
first appears to be at least 67 methods that need to be tested.  However, both of these service
interfaces inherit from <code>rifServices.businessConceptLayer.RIFStudyServiceAPI</code>, which has
10 methods.
</p>

<p>
It is possible that completely different classes could implement each service, thereby making it 
necessary for the 10 methods in <code>RIFStudyServiceAPI</code> to be tested in each of the service 
classes.  If we made no assumption about the underlying classes that implement these services and
tested the ten methods twice, we would be doing what is "black box testing".  In this form of testing,
we guide test case development on what methods are advertised to the test code, not on how those
methods are implemented.
</p>

<p>
However, we know that implementations of these services share a superclass that implements
these methods.  Therefore, if we test one of the shared methods 
<code>getGeoLevelSelectValues(...)</code> in <code>ProductionRIFStudySubmissionService</code>,
we do not need to test the same method in <code>ProductionRIFStudyResultRetrieval</code>.  Here we
are using an aspect of "white box testing", which is using an intimate knowledge of the code base
to create test cases.  We now have to test at least 57 methods, which reduces our testing effort.
</p>

<blockquote>
<font color="green">
<i>
<b>
<a href="SummaryOfDesignDecisions.html#testability4">
Testability-4
</a>
</b>: 
For services interfaces which are likely to have one code implementation, use knowledge of code 
shared between services that would help reduce test scenarios to consider.
</i>
</font>
</blockquote>


<p>
Next, we have to consider how the order of executing these methods could affect the outcome of 
a test case.  For example, if calling <code>methodA()</code> before <code>methodB</code> has a 
different outcome than calling them in the opposite order, then our test suites should contain
test cases that verify behaviour in both these scenarios.  The test cases could be very important,
especially considering that the browser-based clients make asynchronous calls to the web services,
which in turn call the Java-based service classes.
</p>

<p>
If were to choose a sequence of any 2 methods from a possible 57, then we would have 57 choices
for the first method and 56 choices for the second, giving us 3,192 scenarios.  However, will 
use the following assumption to simplify testing:


<blockquote>
<font color="green">
<i>
<b>
<a href="SummaryOfDesignDecisions.html#testability5">
Testability-5
</a>
</b>: 
Limit testing efforts by testing the effect of service methods in isolation rather 
than in combination with one another.  Assume that the order in which the methods are 
called will not affect the outcome.
</i>
</font>
</blockquote>


<p>
We can be more confident in making this assumption if we take the following steps:
<ul>
<li>
isolate service initialisation methods such as <code>initialise(...)</code> and 
<code>isInitialised()</code>, and test them separately.  Clearly, calling these methods
before or after any other method would result in different outcomes
</li>
<li>
every service API test scenario should start by initialising a new service and then calling only a single service method
</li>
<li>
every service API test scenario should end by resetting the service
</li>
<li>
isolate and test separately the scenario where methodA() causes a security exception and that in methodB(),
the user has been black listed.
</li>
</ul>
</p>

<blockquote>
<font color="green">
<i>
<b>
<a href="SummaryOfDesignDecisions.html#testability">
Testability-
</a>
</b>: 
Isolate tests for service methods that would be influenced on the order in which they are called
with respect to other methods.  Then assume that for the remaining service methods, the order in
which they are called will not affect their outcome.
</i>
</font>
</blockquote>



Create one test suite for each service method.  In each suite, only that service method will be
called.


<h3>
Minimising test scenarios for a service method
</h3>
Consider how we would develop test cases for the method <code>getMapAreas()</code>, which is an 
important method used by mapping functions in the Study Submission and Study Result Retrieval tools.

<pre>
   ArrayList<MapArea> getMapAreas(
      User user,
      Geography geography,
      GeoLevelSelect geoLevelSelect,
      GeoLevelArea geoLevelArea,
      GeoLevelToMap geoLevelToMap)
      throws RIFServiceExeption;
</pre>

<p>
In this method, each parameter object can have exactly one of the following five states:
<ul>

<li>
<b>feasible</b>: a non-null value that complies with rules of data type, contains no malicious code and is known in the database.
</li>

<li>
<b>null</b>
</li>

<li>
<b>invalid</b>: contains no malicious code, but the value violates rules of data type
</li>

<li>
<b>non-existent</b>: value complies with data type and contains no malicious code, but is not known
in the database.
</li>

<li>
<b>malicious</b>: contains a String value designed to harm the database or its users
</li>

</ul>

<p>
We could potentially have 5x5x5x5x5 = 3125 test scenarios.  However, we will use the
following assumption to greatly reduce this number:
</p>


<blockquote>
<font color="green">
<i>
<b>
<a href="SummaryOfDesignDecisions.html#testability">
Testability-
</a>
</b>: 
Assume that if we can generate an exception when any one parameter value is infeasible, 
that the service will generate an exception when multiple parameter values are infeasible.
</i>
</font>
</blockquote>

<blockquote>
<font color="green">
<i>
<b>
<a href="SummaryOfDesignDecisions.html#testability">
Testability-
</a>
</b>: 
Assume that in any test method call, we will have at most one service parameter value that 
is infeasible.  When one parameter value is infeasible, all the others will be feasible.
</i>
</font>
</blockquote>

<pre>
minimum test cases
   = 1 scenario where all parameter values are feasible
    + 5 null value scenarios 
    + 5 invalid value scenarios (eg: blank required field)
    + 5 non-existent value scenarios
    + 5 malicious value scenarios
    
    = at least 21
</pre>

<p>
There are two other kinds of test scenarios we could consider:
<ul>
<li>
scenarios where all the values are feasible but in combination they don't make sense
</li>
<li>
scenarios of uncommon behaviour that would produce unusual results
</li>
</ul>

<p>
As an example of the first kind of scenario, consider a version of the RIF database 
that is loaded with geospatial data for both Ireland and UK geographies.  The following
combinations of values are all feasible on their own, but will not make sense when 
considered together:
<ul>
<li>
Geography="Ireland"
</li>
<li>
GeoLevelSelect="Region"
</li>
<li>
GeoLevalArea="Northwest England"
</li>
<li>
GeoLevelToMap="ward"
</li>
</ul>

<p>
As an example of the second kind of scenario, consider parameter values which would 
result in either 0 or 1 area being returned.  This would mark a valid but unusual outcome of 
calling the method.
</p>

<p>
Both of these scenarios depend on the nature of the data set that exists in the database.  
Currently SAHSULAND contains a small, very generic data set that is meant to help demonstrate
features of the RIF Tool Suite.  It lacks enough variety in records to support these either
type of situation.
</p>


<blockquote>
<font color="green">
<i>
<b>
<a href="SummaryOfDesignDecisions.html#testability">
Testability-
</a>
</b>: 
Defer testing for cases where all of the method parameters are feasible in
isolation but together result in an exception.  We will add these test cases when
we produce a more diverse test data set.
</i>
</font>
</blockquote>

<h2>
Organising test code into test suites and test cases
</h2>


Create test packages that mirror dataStorageLayer and businessConceptLayer packages

Create one test class for each business class that can be used as a parameter for a service method.

Create one test class for each service method


Cross-reference results of running test suites for business concepts and service classes.  If 
















The main kinds of test cases we have are: common, uncommon, error and malicious.


Have one test class per business class.

Have one test class per service method.

















</section>

<footer>
<!-- Bottom Footer -->
Copyright (c) Small Area Health Statistics Unit, Imperial College.
</footer>
</body>

</html>