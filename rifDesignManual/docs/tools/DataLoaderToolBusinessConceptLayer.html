<head>
<title>
Data Loader Tool Challenges
</title>

<meta 
	name="author" 
	lang="en" 
	content="Kevin Garwood, Peter Hambly, Margaret Douglass">
<meta 
	name="keywords" 
	lang="en" 
	content="Rapid Inquiry Facility, RIF, environmental health, Kevin Garwood, Java">
	
<link rel="stylesheet" href="../rifDesignManual.css" type="text/css">
</head>

<body>

<header>
<!-- Top Banner -->


<table bgcolor="#B9CDE5">

<tr>
<td width="1000">
<img src="../img/RIFMainBanner.jpg">
</td>
</tr>

<tr>
<td width="1000">

<script src="../lib/jquery-2.1.4.min.js" type="text/javascript"></script>

<script type="text/javascript">
var timeout         = 500;
var closetimer		= 0;
var ddmenuitem      = 0;

function rif_open()
{	rif_canceltimer();
	rif_close();
	ddmenuitem = $(this).find('ul').eq(0).css('visibility', 'visible');}

function rif_close()
{	if(ddmenuitem) ddmenuitem.css('visibility', 'hidden');}

function rif_timer()
{	closetimer = window.setTimeout(rif_close, timeout);}

function rif_canceltimer()
{	if(closetimer)
	{	window.clearTimeout(closetimer);
		closetimer = null;}}

$(document).ready(function()
{	$('#rif > li').bind('mouseover', rif_open);
	$('#rif > li').bind('mouseout',  rif_timer);});

document.onclick = rif_close;
</script>

<ul id="rif">
	<li>
		<a href="../index.html">Project Background</a>
	</li>
	<li>
		<a href="./TooleSuite.html">Tool Suite</a>
	</li>
	<li>
		<a href="../general_architecture/GeneralArchitecture.html">General Architecture</a>
	</li>
	<li>
		<a href="../front_ends/FrontEnds.html">Front Ends</a>
	</li>
	<li>
		<a href="../middleware/Middleware.html">Middleware</a>
	</li>		
	<li><a href="../BackEnds.html">Back Ends</a>
	</li>
	<li><a href="../about_us/AboutUs.html">About Us</a>
	</li>
</ul>

<div class="clear"> </div>
<br>

</td>
</tr>
</table>

</header>

<nav id="sideBarNavigationLinks">
<!-- Left Side Bar -->

<a href="./StudySubmissionTool.html">Study Submission Tool</a>
<br>
<a href="./StudySubmissionTool.html">Study Result Viewer</a>
<br>
<a href="./DataLoaderTool.html">Data Loader Tool</a>
<br>
<a href="./InformationGovernanceTool.html">Information Governance Tool</a>

</nav>

<section>
<!-- Main Content Area -->


<h1>Data Loader Tool Challenges</h1>

<p>
<i>
by <a href="mailto:kgarwood@users.sourceforge.net">Kevin Garwood</a>
</i>
</p>

<p>
The RIF Data Loader Tool is responsible for populating the RIF database with data sets so 
that scientists are able to create studies.  It is a bespoke, domain-specific example of 
what is called an Extract-Transform-Load system.  ETLs are designed to populate a database 
by bringing together data that may come from different data sources, each of which may have 
a different data format and a different level of data quality.
</p>

<p>
ETLs are often complex systems, and their development can be challenging for a number of 
reasons:
<ul>
<li>
the assumptions that developers make about the properties of imported data sets and their 
range of likely field values may not hold when users actually begin to load their files.
</li>
<li>
data loading can involve a lot of configuration details which must cater to the performance 
characteristics of vendor-specific databases and not just the domain-specific uses cases
</li>
<li>
automating the data loading process can involve code generation activities which may prove 
to run less efficiently than bespoke hand-crafted code that would do the same tasks
</li>
<li>
the data loader may have to apply complex information governance rules to a data set.  
The kind of security policies and features that the tool supports could vary from project 
to project.
</li>
<li>
the data loader may have to manage data from a legacy database schema
</li>
</ul>


<h2>Challenges of making assumptions about imported data sets</h2>
As an example of the first challenge, we could make the data loader tool assume that 
a field 'sex' will have integer values of either 1 or 2 but have it encounter a record 
that has a value of 'M'. 'M' may have appeared in a record because of:
<ul>
<li>
<b>changing coding standards</b>: 'M' appeared in an old record from years ago, when the 
coding conventions were 'M' and 'F' rather than 1 and 2
</li>
<li>
<li>
<b>outdated legacy healthcare systems</b>: 'M' is a drop-down list option on an electronic 
hospital form that hasn't been changed in years.
</li>
<li>
<b>accidental data entry error</b>: Someone accidentally typed an 'M' into a free-text 
field on a form which did no relevant validation checks.
</li>
<li>
<b>naive data entry error</b>: Someone typed 'M' into a free-text field and thought it 
was a valid way to specify sex on a form that provided no relevant validation checks
</li>
<li>
<b>program errors</b>: Someone may have entered 1 correctly in a form but a bug in the 
program may have caused the recorded value to be 'M'.
</li>
</ul>

<p>
The assumptions about data quality should be informed by an analysis of data variance in the data sets rather than what might seem to be a reasonable expectation about its values.  
When a system is expected to support data sets from multiple research groups, then either the system will need to relax the assumptions it applies to all data sets or it will need to support more assumptions that are applied to select data sets.  
In trying to appeal to more use cases, designers will often add more complexity to their code.  For example, although the Data Loader Tool could anticipate '0', '1', 'M', 'm', 'F', 'f', 'male', 'MALE', 'FEMALE', 'female', 'transgender', 'unknown', 'UNKNOWN', 'N.A.', it may only ever encounter a few of these possibilities in most settings.  Its cleaning feature would be more robust at the expense of being more complicated and present a potentially higher maintenance cost over the lifetime of the tool.
</p>

<h2>Challenges of making performance of auto-generated query code match hand-crafted 
query code</h2>

<p>
The Data Loader Tool needs to rely heavily on automatically generated database queries 
in order to create, update, combine and select tables and fields.  It is easiest for the 
tool to process imported data sets if they are all processed through the same sequence of 
transformation steps.  However, treating them in the same automated fashion can remove the 
element of human judgement which could be used to publish each one more efficiently.  
</p>
<p> 
For example, data analysts could decide to change the level of detail used to audit data 
cleaning changes, depending on whether they expect a few or a lot of changes to occur.  
Reporting detailed information about a few changes or summary information about thousands 
of changes may both be useful policies to adopt.  However, auditing detailed descriptions 
of thousands of changes could devalue the information that is collected in a change log 
because few people would ever read it in detail.  

<p>
In an automated approach, automatically generated queries could report the same level of 
detail in changes, regardless of whether a lot of changes are made or not.
</p>

<p>
As another example, data analysts who knew a table had exactly the same field names and 
data types as a finished RIF table might decide to skip various steps such as data 
cleaning or mapping data set fields to parts of the RIF schema.  However, an automated 
process would likely still process the data set through all of the steps to ensure that 
all data sets received a uniform treatment.
</p>

<p>
The data loader tool could support a variety of configuration options which provided a 
compromise between the approach that would be favoured by human judgement and the approach 
favoured through machine-based decision making.  For example, each data set field could 
be associated with a level of detail which controlled the extent to which change logs 
audited data cleaning actions. Other options could be created to allow RIF managers to 
skip certain steps used to transform an imported CSV into a published table.
</p>

<p>
However, adding too many configuration options reveals a limitation of model-driven 
software approaches: eventually the effort needed to configure generated features 
approaches the amount of work that would be needed to code a bespoke solution from 
scratch.  Each configuration option presents a new feature for testing, end user training 
and code maintenance.  If configuration options are rarely used in production settings, 
then the features simply add more complexity to an already complex model-driven tool.
</p>

<h2>Challenges of catering to vendor-specific databases</h2>
<p>
The data loader tool uses SQL to tell the database how to create and modify tables that 
hold imported data.  SQL stands for Standard Query Language and in theory it should allow 
the same query to be run on multiple types of databases.  In practice, different database 
vendors support the SQL standard to different extents, and some have additional features 
which may not be portable from one kind of database to the next.
</p>

<p>
PostgreSQL and SQL Server, the two main databases that are expected to be supported in 
production, have various types of differences:
<ul>
<li>
syntax of queries:  for example, the syntax for creating a table from a select statement is different.  These are used a lot in data loading activities to create temporary tables.
</li>
<li>
support for data types: for example, PostgreSQL supports BOOLEAN and TEXT types, whereas MS Server does not.
</li>
<li>
support for regular expressions: for example, search operations could use a regular expression pattern to find anything that starts with a phrase.  This can be important in RIF operations that are searching for health codes in numerator data.
</li>
</ul>

<p>
The web site <a href="http://www.pg-versus-ms.com">PostgreSQL vs MS SQL Server"</a> provides 
an excellent comparison between the two database technologies.  In general, the differences 
between database technologies cause the following consequences for designing the data loader 
tool:
<ul>
<li>
The RIF cannot take advantage of special features that exist in one supported database 
technology but not the other
</li>
<li>
database features which are supported in one but not both of PostgreSQL and SQL Server 
must be implemented as Java code in the middleware
</li>
<li>
The code base for the RIF may need to support a lot of highly similar pieces of code which 
support the same kind of database action but which cater to different target databases
</li>
</ul>

<p>
One of the most significant implications of these consequences is that we cannot take 
advantage of the rich features provided by PostGIS, which is only supported in PostgreSQL.  
This decision implies a lot of software development work to support the features uniformly 
in the middleware.
</p>

<h2>Challenge to support information governance rules</h2>
Because the RIF database manages identifiable and therefore very sensitive health data, 
there is a risk that unauthorised users could access the results of running a study or 
the data tables that are used to produce them.  Two key design question we must ask are: 
<ul>
<li>
To what extent should the Data Loader Tool anonymise, pseudonymise or omit sensitive 
imported data? 
</li>
<li>
To what extent should the Data Loader Tool control the sensitivity of information that 
appears in change audit logs?
</li>
<li>
How long should the intermediary tables used to transform an imported data set into a RIF 
published data set be retained?
</li>
</ul>

<p>
We identified three different responses to these questions.
</p>

<h3>Approach 1: Provide no support for encrypting or omitting sensitive data.</h3>
The main features of this approach are:
<ul>
<li>
RIF managers are responsible for encrypting or omitting sensitive fields in a data set 
before they use the RIF Data Loader Tool to import it.
</li>
<li>
Sensitive data may appear in intermediary tables created during data loading and the 
final published data set that is made visible to RIF scientists
</li>
<li>
Change logs may record sensitive information, (eg: <code>"NHS number was changed from 
123-456-7890 to null"</code>)
</li>
<li>
intermediary tables which are created during the data loading process are retained, 
either to satisfy auditing purposes or to be used later on in other data loading 
activities.  RIF managers must manually delete these tables.
</li>

<p>
For example, consider importing a data set showing individual episodes of treating cancer.  
If the RIF manager makes no effort to encrypt patient identifiers, they will be preserved 
in a number of intermediary tables as well as the published table that is available to 
end-users.  Changes to sensitive fields will appear in a change table and can include 
statements like: <code>"NHS number was changed from 123-456-7890 to null"</code>. Even 
after a data set has been published, temporary tables containing cleaned data will be 
retained in case they can be combined wither other imported files to make different 
data sets.  Intermediary tables such as these may contain patient identifiers or other 
fields that could be used to identify a person.
<p>
In this approach, project managers rely on the security of their computer networks, 
institution-based data access agreement forms and their own manual intervention to manage 
sensitive data.  This approach is the cheapest option for RIF development because the burden 
of protecting sensitive data is pushed onto human users and security features which are 
inherent either in the underlying database vendor or in the corporate computer network.  
However, adding better data protection features in the RIF can help projects which operate 
in countries that expect them to exercise high levels of due diligence to prevent 
inappropriate disclosures of sensitive data.
</p>

<h3>Approach 2: Encrypt or omit sensitive data only in published tables</h3>
This approach provides only a little more data protection facilities:  
<ul>
<li>
Provide features in the Data Loader Tool which allow RIF managers to encrypt or omit 
fields in published data sets.
</li>
<li>
Sensitive information will still appear in change logs and intermediary tables used to 
produce the published data sets.
</li>
<li>
Intermediary tables will exist until RIF managers manually delete them.
</li>

<p>
Following our previous example, NHS numbers would be retained in all the intermediary 
tables but the Data Loader Tool would allow RIF managers to encrypt the field in the 
final published data set.  However, the change "NHS number was changed from 123-456-7890 
to null" could still appear in the change logs.  Unauthorised parties who access the 
database could inspect the intermediary cleaned table and observe sensitive data.
</p>

<h3>
Approach 3: Encrypt or omit sensitive data in change logs and all intermediary and 
published tables.
</h3>

<p>
This approach is the most expensive because it takes on responsibilities the other two 
approaches may have delegated to people.  As well, it assumes that an unauthorised user 
may gain access to the RIF database and minimises the amount of sensitive information 
that are stored in it.  Its features are:
<ul>
<li>
allows RIF managers to encrypt or omit fields as early as possible in the data 
loading process.  Subsequent intermediary tables will inherit the effects of protecting 
data in earlier steps
</li>
<li>
allows RIF managers to customise the extent to which sensitive information is 
recorded for changes made to each field
</li>
<li>
removes temporary tables as soon as possible and will not store imported data in 
their original state
</li>
</ul>

<h3>Automatically encrypt sensitive data types and hold no sensitive change log data</h3>
In this last approach, policies for encrypting sensitive data and recording changes 
are automated and non-configurable.  When fields from an imported data set are 
associated with RIF data types such as "UK Postal Code" or "NHS Number", they are 
automatically encrypted.  All field changes will be of the form "Field X was changed" 
rather than "Field X was changed from [potentially sensitive old value] to 
[potentially sensitive new value].  Like in the previous approach, intermediary tables 
will be deleted as soon as possible.

<p>
This approach has the advantage that there are fewer features for the RIF manager to 
learn or have the responsibility to manage.  However, some projects may find this 
approach too strict.  As well, if they rely more on institutional data access agreements 
rather than advanced software security features, they may want to retain sensitive 
information to make their analyses easier to make.
</p>
<p>
Each of these approaches could appeal to different groups.   However, we are making one 
product and we have to know which one or combination of approaches to use in a way that 
minimises development costs.
</p>

<h2>Support for Legacy Database Schemas</h2>
If a free, open-source, generic ETL tool existed which could transform RIF data sets, we 
would likely consider using it.  However, it is often the case that generic tools have 
trouble working within the constraints of legacy database schemas.  The RIF database 
schema has evolved over a period of over a decade and has now been generalised so that 
it may be used in other projects.   However, it may have idiosyncracies which may not be 
anticipated by generic tools.  Trying to get an off-the-shelf ETL tool to work with RIF 
data could involve: conducting a detailed analysis of its features to determine its 
suitability; performing intensive testing to ensure that the tool does what it claims 
it does; and writing scripts to help RIF data comply with the tool or vice versa. 

<h2>Towards a bespoke solution for a Data Loader Tool</h2>
In view of the challenges we have discussed, we decided that it was better to try and 
develop a bespoke, RIF-specific ETL tool rather than trying to shop for an existing 
solution.  This decision incurs a high development cost, but ensures that the tool can 
fit within the processes we have developed to import and transform population health data 
and geospatial data.

<p>
<i>
by <a href="mailto:kgarwood@users.sourceforge.net">Kevin Garwood</a>
</i>
</p>

</section>

<footer>
<!-- Bottom Footer -->
Copyright (c) Small Area Health Statistics Unit, Imperial College.
</footer>



</body>
</html>